{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a9c3d9",
   "metadata": {},
   "source": [
    "# Building your own recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938b55e5-2dec-486f-a567-b3819088be5c",
   "metadata": {},
   "source": [
    "* This recommender system uses `cosine similarity values` to provide users with top 0-5 books which are most likely to belong to the **genre** they want "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f892276-c080-496b-b1a9-3dca41bdc8e5",
   "metadata": {},
   "source": [
    "* To present the results in a dataframe, import `pandas`\n",
    "* To use cosine similarity, import `sklearn` and `cosine_similarity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef49deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b9385b",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cb74c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'google_books_1299.csv' # This is the file which contains all the data we need to use\n",
    "\n",
    "# Writes a function 'get_data' to read data, get lowercased words\n",
    "# and drop all the missing values into a dataframe\n",
    "def get_data(path_to_data):\n",
    "\n",
    "    data = pd.read_csv(f'{path_to_data}',index_col=0) # reads data from the file 'google_books_1299.csv' into the dataframe 'data'\n",
    "    # index is the column [0] (the first column, which is the 'title' column in the dataframe)\n",
    "    \n",
    "    data[\"title\"] = data[\"title\"].str.lower() # Converts every characters of every entry in the 'title' column to lowercase\n",
    "    data = data.dropna() # drops all the missing values\n",
    "    data.index = [i for i in range(0,len(data))] # sets the data index as the length of this dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec99a1d-b8b7-4b40-9f6a-deca20ec4ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the function 'get_data' on the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "281cda20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16337112",
   "metadata": {},
   "source": [
    "# Create a content-based recommender system using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e66347df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This recommender system uses the simple_preprocess package\n",
    "# So at first we import this package\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "data['generes'] = data['generes'].astype('string') # 1, Converts the datatype of every value in the column 'genres' to string\n",
    "data['preprocessed_genre'] = data['generes'].apply(\n",
    "    lambda genre: simple_preprocess(genre.replace('&', '').replace('amp', '').replace(',', '').replace('none', ''), min_len=3) if type(genre) is str else '')\n",
    "\n",
    "# 2, Creates a new column 'preprocessed_genre', copys all the values from 'generes' to this new column\n",
    "\n",
    "# Tokenizes every entry in the column 'preprocessed_genre'\n",
    "# Removes ',' '&' and 'amp' between words, convert NaN into ''\n",
    "# min_len=1 means the token should be at least 1 character\n",
    "# If the datatype of any entry inside this column is not string, just return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2567526b-6145-4761-a67e-a79518c6f526",
   "metadata": {},
   "source": [
    "* For the next step, we need to use `CountVectorizer` and `TfidfVectorizer`\n",
    "* Also we need to import `cosine_similarity` from `sklearn`, and `scipy` from `spatial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbaa19d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b578ca9-6fd7-4726-836e-4024359f5d07",
   "metadata": {},
   "source": [
    "* Before calculating the cosine similarity, we install the `SentenceTransformer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63cbe1f-b3b8-43b6-b050-a629b3b3965f",
   "metadata": {},
   "source": [
    "* We choose to use the SentenceTransformer model `all-MiniLM-L6-v2`.\n",
    "* This model maps sentences and paragraphs to a 384 dimensional dense vector space, and can be used for tasks like clustering or semantic search. <https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc225ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This framework provides an easy method to compute dense vector representations for sentences.The models are based on transformer networks like BERT.Text is embedding in vector space such that similar text is close and can efficiently be found using cosine similarity.\n",
    "from sentence_transformers import SentenceTransformer \n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e949ac-e8df-4628-8727-5cfd6730b55b",
   "metadata": {},
   "source": [
    "* The recommender contains 2 functions:\n",
    "    * The first function finds the resulst according to `cosine similarity values`, and present the **top (0-5) results** which has the **highest** cosine similarity values\n",
    "    * The second function provides the users a dictionary of all the **genres** of books in the dataset, so that users can **type in** what genre they want\n",
    "        * Then, the second functions applies the first function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45560a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1:\n",
    "# This part is built to sort the books and recommend the books to users\n",
    "# Users cannot see this part\n",
    "\n",
    "def recommender(genres, data): # Creates a function called 'recommender' \n",
    "    book_dic = {} # Creates an empty dictionary\n",
    "    \n",
    "    # We want to apply this function row by row in the dataframe, so:\n",
    "    \n",
    "    for index, value in data.iterrows(): #.iterrows: Iterates over DataFrame rows as (index, Series) pairs. -reads the DataFrame row by row\n",
    "        genres_embeddings = model.encode([genres]) # Uses the model to encode genres with UTF-8\n",
    "        value_embeddings = model.encode(', '.join([str(elem) for elem in value[13]])) #encodes the string values in genres\n",
    "        cosine = 1 - spatial.distance.cosine(genres_embeddings, value_embeddings) # cosine similarity = 1 - cosine distance between the 'genres_embeddings' and 'value_embeddings' \n",
    "        if cosine > 0.7: # When cosine similarity is large (> 0.7)\n",
    "            \n",
    "            # Because this is a loop function:\n",
    "            # if the rating value (value[2]) has already existed in the dictionary, we don't need to import them, and we can just go on \n",
    "            if index in book_dic.keys(): \n",
    "                continue\n",
    "                \n",
    "            # if the rating value is not yet in the dictionary, we add this value as a float number to the dictionary\n",
    "            else:\n",
    "                book_dic[index] = float(value[2]) \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # Then, there can be three conditions when users type the genre they want into this recommender:\n",
    "    \n",
    "    # 1st condition: when there are more than 5 books qualified for the search\n",
    "    if book_dic and len(list(book_dic.keys()))>=5: \n",
    "        book_dic = dict(sorted(book_dic.items(), key=lambda x: x[1], reverse=True)) # lists the results from the highest to the lowest cosine similarity values\n",
    "        book_indexs = list(book_dic.keys())[:5] # picks the top 5 results (which have the highest cosine similarity values)\n",
    "        recommendation = pd.DataFrame(columns=['ISBN','title']) # presents the top 5 results by showing their 'ISBN's and 'title's in the dataframe\n",
    "        \n",
    "        # Use a small for loop to create a list of dictionary wich contains the 'ISBN's and 'title's of the recommended books\n",
    "        for book_index in book_indexs:\n",
    "            book_ISBN = data['ISBN'].iloc[book_index] # maps the 'ISBN' according to the book index\n",
    "            book_title = data['title'].iloc[book_index] # maps the 'title' according to the book index\n",
    "            recommendation=recommendation.append({'ISBN' : book_ISBN , 'title' : book_title} , ignore_index=True) # appends each recommended book to the list     \n",
    "        return recommendation\n",
    "    \n",
    "    # 2nd condition: when there are less than 5 books qualified for the search\n",
    "    elif book_dic:\n",
    "        # Gives an explanation to the users\n",
    "        # The rest of the codes remain the same as the 1st condition, because there are only fewer books\n",
    "        print(\"We can only give you less than 5 recommendations\") \n",
    "        book_dic = dict(sorted(book_dic.items(), key=lambda x: x[1], reverse=True))\n",
    "        book_indexs = list(book_dic.keys())\n",
    "        recommendation = pd.DataFrame(columns=['ISBN','title'])\n",
    "        for book_index in book_indexs:\n",
    "            book_ISBN = data['ISBN'].iloc[book_index]\n",
    "            book_title = data['title'].iloc[book_index]\n",
    "            recommendation=recommendation.append({'ISBN' : book_ISBN , 'title' : book_title} , ignore_index=True)      \n",
    "        return recommendation\n",
    "        \n",
    "    # 3rd condition: when there are no results\n",
    "    else:\n",
    "        # Gives an explanation to the users\n",
    "        recommendation = \"Sorry, we do not have any recommentation for you.\"\n",
    "        return recommendation\n",
    "\n",
    "# Function 2:\n",
    "# This part is built to present users with all the genres\n",
    "# So that users can select which genre they want\n",
    "\n",
    "# This is what the users actually confront with\n",
    "\n",
    "def Content_based_recommender(data): # Creates another function called 'Content_based_recommender' \n",
    "\n",
    "    data = data[data['preprocessed_genre'].notna()] # drops the missing values in the column 'preprocessed_genre'\n",
    "    genres_set = set() # get every unique value from the column 'preprocessed_genre' into a dictionary 'genres_set'\n",
    "    for i in data['preprocessed_genre']: # iterate on every single word in this column\n",
    "        for genre in i:\n",
    "            genres_set.add(genre) # Every unique value is a genre, and thus is added to the dictionary\n",
    "    \n",
    "    # Now the users will be shown with the dictionary which contains all the genres in the dataframe\n",
    "    # They are asked to type in the genre they want\n",
    "    print(f\"What type of genre do you like? \\n\\nYou can choose from the following:\\n\\n{genres_set}\")\n",
    "    genres = input().lower() # Lowercase the user's input\n",
    "    \n",
    "    # Applies the previous function 'recommender' to find the recommended books from the dataframe\n",
    "    # The result is stored as 'recommendations'\n",
    "    recommendations = recommender(genres, data)\n",
    "\n",
    "    # Shows the recommendations\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd280c5a-3b24-4eb5-b716-00c2e91aab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can run our recommender!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23f52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Content_based_recommender(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaaf4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a639c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9fbf10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cab222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe2f088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
